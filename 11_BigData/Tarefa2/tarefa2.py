# -*- coding: utf-8 -*-
"""Tarefa2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NsznR0E4bIVuf-aafEeoPZmEZiERe3xK

# Big Data: Como instalar o PySpark no Google Colab

Como instalar o PySpark no Google Colab é uma dúvida comum entre aqueles que estão migrando seus projetos de Data Science para ambientes na nuvem.

O termo Big Data está cada vez mais presente, e mesmo projetos pessoais podem assumir uma grande dimensionalidade devido à quantidade de dados disponíveis.

Para analisar grandes volumes de dados, Big Data, com velocidade, o Apache Spark é uma ferramenta muito utilizada, dada a sua capacidade de processamento de dados e computação paralela.

O Spark foi pensado para ser acessível, oferecendo diversas APIs e frameworks em Python, Scala, SQL e diversas outras linguagens.

## PySpark no Google Colab

PySpark é a interface alto nível que permite você conseguir acessar e usar o Spark por meio da linguagem Python. Usando o PySpark, você consegue escrever todo o seu código usando apenas o nosso estilo Python de escrever código.

Já o Google Colab é uma ferramenta incrível, poderosa e gratuita – com suporte de GPU inclusive. Uma vez que roda 100% na nuvem, você não tem a necessidade de instalar qualquer coisa na sua própria máquina.

No entanto, apesar da maioria das bibliotecas de Data Science estarem previamente instaladas no Colab, o mesmo não acontece com o PySpark. Para conseguir usar o PySpark é necessário alguns passos intermediários, que não são triviais para aqueles que estão começando.

Dessa maneira, preparei um tutorial simples e direto ensinando a instalar as dependências e a biblioteca.

## Instalando o PySpark no Google Colab

Instalar o PySpark não é um processo direto como de praxe em Python. Não basta usar um pip install apenas. Na verdade, antes de tudo é necessário instalar dependências como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7.
"""

# instalar as dependências
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

"""A próxima etapa é configurar as variáveis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependências estão rodando.

Para conseguir “manipular” o terminal e interagir como ele, você pode usar a biblioteca os.
"""

# configurar as variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

# tornar o pyspark "importável"
import findspark
findspark.init('spark-2.4.4-bin-hadoop2.7')

import pyspark

#sc.stop()

#sc = pyspark.SparkContext.getOrCreate()
sc = pyspark.SparkContext(appName='BigDataTarefa2')

flights_file = '/content/flights.csv'
flights_RDD = sc.textFile(flights_file)

# a. Tempo total de voo de cada companhia
def TempoTotalVooCia(line):
    record = line.split(',')
    return (record[1],float(record[9]))

print(flights_RDD.map(TempoTotalVooCia).reduceByKey(lambda a,b: a+b).take(10))

# b. Destino mais visado
def DestinoMaisVisado(line):
    record = line.split(',')
    return (record[4],record[1])

tuple = flights_RDD.map(DestinoMaisVisado).collect()
tuple_RDD = sc.parallelize(tuple)
#print(tuple_RDD.countByKey())

maxKey = tuple_RDD.max()
print(f'Destino mais visado: {maxKey}')

# c. Quais aeroportos cada companhia passou
def aeroportosOrigem(line):
    record = line.split(',')
    return (record[1],record[3])

def aeroportosDestino(line):
    record = line.split(',')
    return (record[1],record[4])

listaOrigem = flights_RDD.map(aeroportosOrigem).take(90000)
listaDestino = flights_RDD.map(aeroportosDestino).take(90000)

listaOrigem_RDD = sc.parallelize(listaOrigem)
listaDestino_RDD = sc.parallelize(listaDestino)

grouped_elementsDestino = listaOrigem_RDD.union(listaDestino_RDD).distinct().groupByKey().take(90000)
for key, values in grouped_elementsDestino:    
    print(key, list(values))

# d. Vôo de maior distância de cada companhia
def DistanciasPorCia(line):
    record = line.split(',')
    return (record[1],float(record[10]))

distancias = flights_RDD.map(DistanciasPorCia).collect()
maxDist_RDD = sc.parallelize(distancias).reduceByKey(max).collect()
print(maxDist_RDD)

# e. Qual é o vôo mais frequente de cada companhia
def voos(line):
    record = line.split(',')
    return (record[1],record[3]+record[4])

lista = flights_RDD.map(voos).collect()
lista_RDD = sc.parallelize(lista)

grouped_elements = lista_RDD.groupByKey().collect()
qtdVoosPorCia = lista_RDD.map(lambda x: (x, 1)).reduceByKey(lambda a,b: a+b).groupByKey().collect()
print(qtdVoosPorCia)

for key, values in qtdVoosPorCia:
    print(key, list(values))

# Quantidade de cada vôo por companhia
def voos(line):
    record = line.split(',')
    return (record[1],record[3]+record[4])

lista = flights_RDD.map(voos).collect()
lista_RDD = sc.parallelize(lista)

grouped_elements = lista_RDD.groupByKey().collect()

print(lista_RDD.map(lambda x: (x, 1)).reduceByKey(lambda a,b: a+b).collect())

# Vôo de cada companhia
def voos(line):
    record = line.split(',')
    return (record[1],record[3]+record[4])

lista = flights_RDD.map(voos).take(90000)
lista_RDD = sc.parallelize(lista)

grouped_elements = lista_RDD.distinct().groupByKey().take(90000)
for key, values in grouped_elements:    
    print(key, list(values))

import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')

shakespeare_file = '/content/shakespeare_teste.txt'

stopwords = nltk.corpus.stopwords.words('english')
punctuation = string.punctuation

contador=0
listaLinha=[]
arrayToken=[]

shakespeare_RDD = sc.textFile(shakespeare_file)
print(shakespeare_RDD.collect())

key_tokens_RDD = sorted(tokens_RDD.map(lambda x: x).distinct().collect())
print(key_tokens_RDD)

count=0
lista=[]
for i in range(0,shakespeare_RDD.count()):                
    print(shakespeare_RDD.collect()[i])
    contador=[]
    for p in shakespeare_RDD.collect()[i].split(' '):                        
        count+=1        
        if p not in lista:
            print(f'Lista:{lista}')
            print(f'P:{p}')            
            lista.append(p)     
            contador.append(i)
            lista.append(i)
        else:
            contador.append(i) 
            
            i+=1
print(lista)

def tokenizer(line):
    for token in nltk.word_tokenize(line):
        if token not in stopwords and token not in punctuation:
            yield token.lower()

grouped_elements = sc.parallelize(["a", "a", "c", "a"], 3).zipWithIndex().groupByKey.collect()
for key, values in grouped_elements:    
    print(key, list(values))

class Mapper(object):

    def __init__(self, line, separator='\t'):
        self.infile = line
        self.sep = separator

    def emit(self, key, value):
        sys.stdout.write(f"{key}{self.sep}{value}\n")

    def map(self):
        for line in self:
            cur_line = 1
            arr_words = []
            for word in line.split():
                word = ''.join(e for e in word if e.isalnum()).lower()

                if word not in arr_words:
                    arr_words.append(word)
                    self.emit(word, cur_line)
                    cur_line += 1
                
    def __iter__(self):
        for line in self.infile:
            yield line.split(self.sep, 1)[1]


def tokenizer(line):
    for token in nltk.word_tokenize(line):
        if token not in stopwords and token not in punctuation:
            yield token.lower()

shakespeare_RDD = sc.textFile(shakespeare_file)
print(shakespeare_RDD.collect())

 

if __name__ == "__main__":
    mapper = Mapper(shakespeare_RDD)
    print(mapper.map())

def tokenizer(line):
    for token in nltk.word_tokenize(line):
        if token not in stopwords and token not in punctuation:
            yield token.lower()

shakespeare_RDD = sc.textFile(shakespeare_file)
print(shakespeare_RDD.collect())


tokens_RDD = shakespeare_RDD.flatMap(tokenizer)
print(tokens_RDD.collect())

class FuncIter(object):
    def __init__(self, startVal, continueProc, changeProc):
        # Store all relevant stuff.
        self.currVal = startVal
        self.continueProc = continueProc
        self.changeProc = changeProc

    def __iter__(self):
        return self

    def __next__(self):
        # If allowed to continue, return current and prepare next.

        if self.continueProc(self.currVal):
            retVal = self.currVal
            self.currVal = self.changeProc(self.currVal)
            return retVal

        # We're done, stop iterator.

        raise StopIteration()

print([item for item in FuncIter(1, lambda x: x <= 200, lambda x: x * 4 - 1)])

tokens_RDD = shakespeare_RDD.flatMap(lambda fc: ((fc[0], s) for s in fc[1].lower().split()))
print(tokens_RDD.collect())

tokens_RDD = shakespeare_RDD.flatMapValues(lambda content: content.lower().split(' ')).take(3)
print(tokens_RDD)

shakespeare_RDD = sc.textFile(shakespeare_file)
print(shakespeare_RDD.collect())

print([item for item in shakespeare_RDD.flatMap(1, lambda x: x <= 200, lambda x: x * 4 - 1)])

class FuncIter(object):
    def __init__(self, startVal, continueProc, changeProc):
        # Store all relevant stuff.

        self.currVal = startVal
        self.continueProc = continueProc
        self.changeProc = changeProc

    def __iter__(self):
        return self

    def __next__(self):
        # If allowed to continue, return current and prepare next.

        if self.continueProc(self.currVal):
            retVal = self.currVal
            self.currVal = self.changeProc(self.currVal)
            return retVal

        # We're done, stop iterator.

        raise StopIteration()

print([item for item in FuncIter(1, lambda x: x <= 200, lambda x: x * 4 - 1)])

# c. Quais aeroportos cada companhia passou
def aeroportosOrigem(line):
    record = line.split(',')
    return (record[1],record[3])

def aeroportosDestino(line):
    record = line.split(',')
    return (record[1],record[4])

listaOrigem = flights_RDD.map(aeroportosOrigem).take(90000)
listaDestino = flights_RDD.map(aeroportosDestino).take(90000)

listaOrigem_RDD = sc.parallelize(listaOrigem)
listaDestino_RDD = sc.parallelize(listaDestino)

grouped_elementsDestino = listaOrigem_RDD.union(listaDestino_RDD).distinct().groupByKey().take(90000)
for key, values in grouped_elementsDestino:    
    print(key, list(values))